{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Setup & Imports"
      ],
      "metadata": {
        "id": "6bWrON4PKlH-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1SRZF2pKXvW",
        "outputId": "bf137c90-0755-4cd3-9c9b-90fdb1b64a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# General utilities\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Set seed for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation"
      ],
      "metadata": {
        "id": "gWWlAFs6Kqm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"cleaned_ADHI.csv\")\n",
        "\n",
        "# Filter valid rows\n",
        "required_cols = [\"Catchment\", \"Mean_annual_precip\", \"lc_urban\", \"Maxi_q\", \"q95th\", \"Country\"]\n",
        "df = df.dropna(subset=required_cols)\n",
        "\n",
        "# Create binary flood year target\n",
        "df[\"flood_year\"] = (df[\"Maxi_q\"] > df[\"q95th\"]).astype(int)\n",
        "\n",
        "# Normalize features\n",
        "features = [\"Catchment\", \"Mean_annual_precip\", \"lc_urban\"]\n",
        "scaler = StandardScaler()\n",
        "df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "# Group into train/test per country\n",
        "federated_data = {}\n",
        "for country in df[\"Country\"].unique():\n",
        "    country_df = df[df[\"Country\"] == country]\n",
        "    if len(country_df) < 30:\n",
        "        continue  # Skip small datasets\n",
        "\n",
        "    X = country_df[features].values\n",
        "    y = country_df[\"flood_year\"].values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "    federated_data[country] = {\n",
        "        \"train\": (torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)),\n",
        "        \"test\": (torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
        "    }\n",
        "\n",
        "print(f\"Prepared data for {len(federated_data)} countries.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWKspRjdK5xv",
        "outputId": "0f2e60f9-d978-4372-905a-0980d8819e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared data for 20 countries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Definition (Binary Classifier)"
      ],
      "metadata": {
        "id": "Yn6AYI5qLPpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FloodYearClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=3, hidden_dim=16):\n",
        "        super(FloodYearClassifier, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)  # Single output (logit)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)  # output shape: [batch_size]\n"
      ],
      "metadata": {
        "id": "soKvww0hK_5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Federated Training Loop"
      ],
      "metadata": {
        "id": "ywn3aSBmLmMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_federated_model(\n",
        "    federated_data,\n",
        "    input_dim=3,\n",
        "    hidden_dim=16,\n",
        "    num_rounds=5,\n",
        "    local_epochs=3,\n",
        "    batch_size=16,\n",
        "    lr=0.001\n",
        "):\n",
        "    # Initialize the global model\n",
        "    global_model = FloodYearClassifier(input_dim, hidden_dim).to(device)\n",
        "    global_weights = global_model.state_dict()\n",
        "\n",
        "    for round_num in range(num_rounds):\n",
        "        print(f\"\\n Communication Round {round_num + 1}/{num_rounds}\")\n",
        "        local_models = []\n",
        "\n",
        "        for country, data in federated_data.items():\n",
        "            X_train, y_train = data[\"train\"]\n",
        "\n",
        "            # Build local model and load global weights\n",
        "            local_model = FloodYearClassifier(input_dim, hidden_dim).to(device)\n",
        "            local_model.load_state_dict(global_weights)\n",
        "            local_model.train()\n",
        "\n",
        "            optimizer = torch.optim.Adam(local_model.parameters(), lr=lr)\n",
        "            criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "            dataset = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "            # Train locally\n",
        "            for epoch in range(local_epochs):\n",
        "                for xb, yb in dataset:\n",
        "                    xb, yb = xb.to(device), yb.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    output = local_model(xb)\n",
        "                    loss = criterion(output, yb)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            local_models.append(local_model.state_dict())\n",
        "\n",
        "        # Federated Averaging (FedAvg)\n",
        "        new_state_dict = {}\n",
        "        for key in global_weights.keys():\n",
        "            new_state_dict[key] = torch.stack([local_model[key] for local_model in local_models], dim=0).mean(dim=0)\n",
        "\n",
        "        global_model.load_state_dict(new_state_dict)\n",
        "        global_weights = global_model.state_dict()\n",
        "\n",
        "        print(\"Updated global model.\")\n",
        "\n",
        "    return global_model\n"
      ],
      "metadata": {
        "id": "i2OCWps3Lhdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation & Saving"
      ],
      "metadata": {
        "id": "UkXWi4tvL0u3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate_federated_model(model, federated_data):\n",
        "    model.eval()\n",
        "    all_metrics = []\n",
        "\n",
        "    print(\"\\n Evaluation per country:\")\n",
        "    for country, data in federated_data.items():\n",
        "        X_test, y_test = data[\"test\"]\n",
        "        with torch.no_grad():\n",
        "            preds = model(X_test.to(device))\n",
        "            preds = torch.sigmoid(preds).cpu().numpy()\n",
        "            preds_binary = (preds >= 0.5).astype(int)\n",
        "\n",
        "        y_true = y_test.cpu().numpy()\n",
        "\n",
        "        acc = accuracy_score(y_true, preds_binary)\n",
        "        prec = precision_score(y_true, preds_binary)\n",
        "        rec = recall_score(y_true, preds_binary)\n",
        "        f1 = f1_score(y_true, preds_binary)\n",
        "\n",
        "        all_metrics.append({\n",
        "            \"Country\": country,\n",
        "            \"Accuracy\": acc,\n",
        "            \"Precision\": prec,\n",
        "            \"Recall\": rec,\n",
        "            \"F1\": f1\n",
        "        })\n",
        "\n",
        "        print(f\"  {country:20} | Acc: {acc:.2f} | Prec: {prec:.2f} | Rec: {rec:.2f} | F1: {f1:.2f}\")\n",
        "\n",
        "    return pd.DataFrame(all_metrics)\n"
      ],
      "metadata": {
        "id": "sRAlufUgL1Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model (input_dim = 3 features used in flood_year classification)\n",
        "model = FloodYearClassifier(input_dim=3, hidden_dim=16).to(device)"
      ],
      "metadata": {
        "id": "yreHRCT1MI8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "-VlHDXmaMPjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model weights\n",
        "torch.save(model.state_dict(), \"federated_flood_year_model.pt\")\n",
        "print(\"\\n  Model saved as federated_flood_year_model.pt\")"
      ],
      "metadata": {
        "id": "8E2JLeI_L6uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# --- Extract the zip file ---\n",
        "zip_path = \"/content/data_by_country.zip\"\n",
        "extract_dir = \"/content/data_by_country/data_by_country\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(\"Files extracted.\")\n",
        "\n",
        "# --- Define relevant columns ---\n",
        "features = [\"Catchment\", \"Mean_annual_precip\", \"lc_urban\"]\n",
        "target = \"flood_year\"\n",
        "\n",
        "# --- Store all trained models here ---\n",
        "local_models = {}\n",
        "\n",
        "# --- Loop through each country file ---\n",
        "for file_name in os.listdir(extract_dir):\n",
        "    if not file_name.endswith(\".csv\"):\n",
        "        continue\n",
        "\n",
        "    country = file_name.replace(\".csv\", \"\")\n",
        "    file_path = os.path.join(extract_dir, file_name)\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        df = df.dropna(subset=features + [target])\n",
        "        df[features] = StandardScaler().fit_transform(df[features])\n",
        "\n",
        "        X = torch.tensor(df[features].values, dtype=torch.float32)\n",
        "        y = torch.tensor(df[target].values, dtype=torch.float32)\n",
        "\n",
        "        dataset = TensorDataset(X, y)\n",
        "        loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "        # Define local model\n",
        "        model = FloodYearClassifier(input_dim=3, hidden_dim=16).to(device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # Train locally\n",
        "        model.train()\n",
        "        for epoch in range(5):\n",
        "            for xb, yb in loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred = model(xb).squeeze(-1)\n",
        "                loss = loss_fn(pred, yb)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        print(f\"Trained local model for: {country}\")\n",
        "        local_models[country] = model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Skipping {country}: {e}\")\n"
      ],
      "metadata": {
        "id": "3L84DuGpM7pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def fedavg(models_dict):\n",
        "    \"\"\"Federated Averaging of model weights.\"\"\"\n",
        "    models = list(models_dict.values())\n",
        "    global_model = copy.deepcopy(models[0])  # Start from one of the models\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for param in global_model.state_dict():\n",
        "            # Stack and average parameters across models\n",
        "            avg_param = torch.stack([m.state_dict()[param] for m in models], dim=0).mean(dim=0)\n",
        "            global_model.state_dict()[param].copy_(avg_param)\n",
        "\n",
        "    return global_model\n",
        "\n",
        "# Aggregate\n",
        "global_model = fedavg(local_models)\n",
        "\n",
        "# Save the global model\n",
        "torch.save(global_model.state_dict(), \"global_flood_year_model.pt\")\n",
        "print(\"Global Flood Year model saved as 'global_flood_year_model.pt'\")\n"
      ],
      "metadata": {
        "id": "PYi2rRPMNhAW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}